Note:
Model code:

X_train, X_val, y_train, y_val = train_test_split(X_white, normalized_y_white, test_size=0.2, random_state=42)

model_white = models.Sequential([
    Input(shape=(8, 8, 1)),
    layers.Conv2D(filters=64, kernel_size=(3, 3), kernel_regularizer=regularizers.l2(0.0001), padding='same'),
    layers.BatchNormalization(epsilon=1e-5),
    layers.LeakyReLU(0.1),
    
    layers.Conv2D(filters=64, kernel_size=(5, 5), kernel_regularizer=regularizers.l2(0.0001), padding='same'),
    layers.BatchNormalization(epsilon=1e-5),
    layers.LeakyReLU(0.1),

    layers.Conv2D(filters=64, kernel_size=(8, 8), kernel_regularizer=regularizers.l2(0.0001), padding='same'),
    layers.BatchNormalization(epsilon=1e-5),
    layers.LeakyReLU(0.1),
    
    layers.Flatten(),
    
    layers.Dense(1024,kernel_regularizer=regularizers.l2(0.0001)),
    layers.BatchNormalization(epsilon=1e-5),
    layers.LeakyReLU(0.1),
    
    layers.Dense(512,kernel_regularizer=regularizers.l2(0.0001)),
    layers.BatchNormalization(epsilon=1e-5),
    layers.LeakyReLU(0.1),

    layers.Dense(216,kernel_regularizer=regularizers.l2(0.0001)),
    layers.BatchNormalization(epsilon=1e-5),
    layers.LeakyReLU(0.1),
    
    layers.Dense(1, activation='linear')
])

optimizer = optimizers.Adam(learning_rate=0.0004)

# Compile the model
model_white.compile(optimizer=optimizer,
                    loss=losses.HuberLoss(),
                    metrics=['mean_absolute_error'],)

Trained on white chess positions only. Final loss: 0.0007963. Final val_loss: 0.0008.